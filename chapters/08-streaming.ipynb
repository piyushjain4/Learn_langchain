{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9sciiStEX3b"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/08-streaming.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye774qInP4o3"
      },
      "source": [
        "#### LangChain Essentials Course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtsjqmEUP4o4"
      },
      "source": [
        "# Streaming With Langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMSrFSXyP4o4"
      },
      "source": [
        "LangChain is one of the most popular open source libraries for AI Engineers. It's goal is to abstract away the complexity in building AI software, provide easy-to-use building blocks, and make it easier when switching between AI service providers.\n",
        "\n",
        "In this example, we will introduce LangChain's async streaming, allowing us to receive and view the tokens as they are generated by OpenAI's LLM. The use of streaming is typical in conversational interfaces and can provide a more natural experience for users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xS-V4uqsP5nU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b3b00b-e87e-4277-9977-df6677f981d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m954.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.3/333.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  langchain-core==0.3.33 \\\n",
        "  langchain-openai==0.3.3 \\\n",
        "  langchain-community==0.3.16 \\\n",
        "  langsmith==0.3.4 \\\n",
        "  langchain-ollama\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx2DC8OuP4o5"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ We will be using OpenAI for this example allowing us to run everything via API. If you would like to use Ollama instead, check out the [Ollama LangChain Course](https://github.com/aurelio-labs/langchain-course/tree/main/notebooks/ollama).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import userdata\n",
        "Lang_api =userdata.get('Langsmith_api')"
      ],
      "metadata": {
        "id": "0m22GxYKiewf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCIXuavOP4o5"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ If using LangSmith, add your API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sdZXycP7P4o5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# must enter API key\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] =Lang_api\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"aurelioai-langchain-course-streaming-openai\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update && apt install -y pciutils lshw"
      ],
      "metadata": {
        "id": "xMEYZlJ7qg6A",
        "outputId": "a2332e9a-8782-4fb3-a18f-d43505c185aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connected to cloud.r-project.org (3.163.189.100)] [C\u001b[0m\r                                                                                                    \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r                                                                                                    \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r                                                                                                    \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,237 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,692 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,767 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,679 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,962 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,535 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,003 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [82.7 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Fetched 24.4 MB in 9s (2,681 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "30 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpci3 pci.ids usb.ids\n",
            "The following NEW packages will be installed:\n",
            "  libpci3 lshw pci.ids pciutils usb.ids\n",
            "0 upgraded, 5 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 883 kB of archives.\n",
            "After this operation, 3,256 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 pci.ids all 0.0~2022.01.22-1 [251 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpci3 amd64 1:3.7.0-6 [28.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 lshw amd64 02.19.git.2021.06.19.996aaad9c7-2build1 [321 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 pciutils amd64 1:3.7.0-6 [63.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 usb.ids all 2022.04.02-1 [219 kB]\n",
            "Fetched 883 kB in 1s (678 kB/s)\n",
            "Selecting previously unselected package pci.ids.\n",
            "(Reading database ... 126209 files and directories currently installed.)\n",
            "Preparing to unpack .../pci.ids_0.0~2022.01.22-1_all.deb ...\n",
            "Unpacking pci.ids (0.0~2022.01.22-1) ...\n",
            "Selecting previously unselected package libpci3:amd64.\n",
            "Preparing to unpack .../libpci3_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking libpci3:amd64 (1:3.7.0-6) ...\n",
            "Selecting previously unselected package lshw.\n",
            "Preparing to unpack .../lshw_02.19.git.2021.06.19.996aaad9c7-2build1_amd64.deb ...\n",
            "Unpacking lshw (02.19.git.2021.06.19.996aaad9c7-2build1) ...\n",
            "Selecting previously unselected package pciutils.\n",
            "Preparing to unpack .../pciutils_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking pciutils (1:3.7.0-6) ...\n",
            "Selecting previously unselected package usb.ids.\n",
            "Preparing to unpack .../usb.ids_2022.04.02-1_all.deb ...\n",
            "Unpacking usb.ids (2022.04.02-1) ...\n",
            "Setting up pci.ids (0.0~2022.01.22-1) ...\n",
            "Setting up lshw (02.19.git.2021.06.19.996aaad9c7-2build1) ...\n",
            "Setting up usb.ids (2022.04.02-1) ...\n",
            "Setting up libpci3:amd64 (1:3.7.0-6) ...\n",
            "Setting up pciutils (1:3.7.0-6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmJY_lLkP4o5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! curl -fsSL https://ollama.ai/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJec_cxxijyC",
        "outputId": "2f67f93e-6adb-4361-e932-47919d7c0839"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama serve > /dev/null 2>&1 &"
      ],
      "metadata": {
        "id": "7hl7OCUrimZr"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!ollama pull mistral"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inEnqW1pioik",
        "outputId": "1069070b-0283-4c2f-83f5-a6d4292510c4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
            "pulling ff82381e2bea... 100% ▕▏ 4.1 GB                         \u001b[K\n",
            "pulling 43070e2d4e53... 100% ▕▏  11 KB                         \u001b[K\n",
            "pulling 491dfa501e59... 100% ▕▏  801 B                         \u001b[K\n",
            "pulling ed11eda7790d... 100% ▕▏   30 B                         \u001b[K\n",
            "pulling 42347cd80dc8... 100% ▕▏  485 B                         \u001b[K\n",
            "verifying sha256 digest \u001b[K\n",
            "writing manifest \u001b[K\n",
            "success \u001b[K\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "id": "fH1xGab2snT_",
        "outputId": "98104a6d-d4d0-415c-80f4-0932aaba7069",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME              ID              SIZE      MODIFIED      \n",
            "mistral:latest    f974a74358d6    4.1 GB    3 seconds ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from langchain_ollama.chat_models import ChatOllama\n",
        "\n",
        "model_name = \"mistral\"\n",
        "\n",
        "# initialize one LLM with temperature 0.0, this makes the LLM more deterministic\n",
        "llm = ChatOllama(temperature=0.0, model=model_name)\n",
        "creative_llm = ChatOllama(temperature=0.9, model=model_name)"
      ],
      "metadata": {
        "id": "yxKh-ecrisyl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC8CU_GfP4o6"
      },
      "source": [
        "For the LLM, we'll start by initializing our connection to the OpenAI API. We do need an OpenAI API key, which you can get from the [OpenAI platform](https://platform.openai.com/api-keys).\n",
        "\n",
        "We will use the `gpt-4o-mini` model with a `temperature` of `0.0`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dHdcekEFP4o6"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# from getpass import getpass\n",
        "# from langchain_openai import ChatOpenAI\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \\\n",
        "#     or getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "# llm = ChatOpenAI(\n",
        "#     model_name=\"gpt-4o\",\n",
        "#     temperature=0.0,\n",
        "#     streaming=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4ZveEYgIP4o6"
      },
      "outputs": [],
      "source": [
        "# llm_out = llm.invoke(\"Hello there\")\n",
        "# llm_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPFk5sM8P4o6"
      },
      "source": [
        "## Streaming with `astream`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TRMvz7VP4o6"
      },
      "source": [
        "We will start by creating a aysnc stream from our LLM. We do this within an `async for` loop, allowing us to iterate through the chunks of data and use them as soon as the async `astream` method returns the tokens to us. By adding a pipe character `|` we can see the individual tokens that are generated. We set `flush` equal to `True` as this forces immediate output to the console, resulting in smoother streaming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZjsC9uUP4o6",
        "outputId": "ba66ab76-4fbd-4171-e7b1-60bd44b2ca3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " N|LP| stands| for| Natural| Language| Process|ing|.| It| is| a| field| of| study| in| artificial| intelligence| (|AI|)| and| comput|ational| lingu|istics| that| focuses| on| the| interaction| between| computers| and| humans| using| natural| language|.| The| goal| of| N|LP| is| to| enable| computers| to| understand|,| interpret|,| and| generate| human| language| in| a| valuable| way|.| This| includes| tasks| such| as| speech| recognition|,| text|-|to|-|spe|ech| synt|hesis|,| sentiment| analysis|,| machine| translation|,| and| more|.| N|LP| algorithms| use| statistical| models|,| machine| learning| techniques|,| and| deep| learning| architect|ures| to| analyze| and| process| large| amounts| of| natural| language| data|.||"
          ]
        }
      ],
      "source": [
        "tokens = []\n",
        "async for token in llm.astream(\"What is NLP?\"):\n",
        "    tokens.append(token)\n",
        "    print(token.content, end=\"|\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8lAdYaIP4o6"
      },
      "source": [
        "Since we appended each token to the `tokens` list, we can also see what is inside each and every token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GAzSRmzP4o7",
        "outputId": "374f6c5a-8b68-4b28-db7a-26b245aaf875"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessageChunk(content=' N', additional_kwargs={}, response_metadata={}, id='run-e5ca200d-4269-47f5-8188-f416ae690602')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "tokens[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHj3Iy1FP4o7",
        "outputId": "f4827b08-1936-4adb-a38a-04b26e053730"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessageChunk(content='LP', additional_kwargs={}, response_metadata={}, id='run-e5ca200d-4269-47f5-8188-f416ae690602')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "tokens[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPasT2EMP4o7"
      },
      "source": [
        "We can also merge multiple `AIMessageChunk` objects together with the `+` operator, creating a larger set of tokens / chunk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXO4Fk2CP4o7",
        "outputId": "fe291797-e637-4ad3-baf0-94f32b106200"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessageChunk(content=' NLP stands for Natural', additional_kwargs={}, response_metadata={}, id='run-e5ca200d-4269-47f5-8188-f416ae690602')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "tokens[0] + tokens[1] + tokens[2] + tokens[3] + tokens[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDgHe0V-P4o7"
      },
      "source": [
        "A word of caution, there is nothing preventing you from merging tokens in the incorrect order, so be cautious to not output any token omelettes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpwg02OHP4o7",
        "outputId": "4796f65b-210c-4b33-b192-40a8a421918c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessageChunk(content=' Natural for standsLP N', additional_kwargs={}, response_metadata={}, id='run-e5ca200d-4269-47f5-8188-f416ae690602')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "tokens[4] + tokens[3] + tokens[2] + tokens[1] + tokens[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPBj3ryzP4o7"
      },
      "source": [
        "## Streaming with Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdXrkOkQP4o7"
      },
      "source": [
        "Streaming with agents, particularly the custom agent executor, is a little more complex. Let's begin by constructor a simple agent executor matching what we built in the [Agent Executor](https://github.com/aurelio-labs/langchain-course/blob/main/notebooks/openai/07-custom-agent-executor.ipynb) chapter.\n",
        "\n",
        "To construct the agent executor we need:\n",
        "\n",
        "* Tools\n",
        "* `ChatPromptTemplate`\n",
        "* Our LLM (already defined with `llm`)\n",
        "* An agent\n",
        "* Finally, the agent executor\n",
        "\n",
        "Let's start defining each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b97_81lTP4o7"
      },
      "source": [
        "### Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0T2lL7ZP4o7"
      },
      "source": [
        "Now we will define a few tools to be used by an async agent executor. Our goal for tool-use in regards to streaming are:\n",
        "\n",
        "* The tool-use steps will be streamed in one big chunk, ie we do not return the tool use information token-by-token but instead it streams message-by-message.\n",
        "\n",
        "* The final LLM output _will_ be streamed token-by-token as we saw above.\n",
        "\n",
        "For these we need to define a few math tools and our final answer tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "w0XZOB6GP4o7"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def add(x: float, y: float) -> float:\n",
        "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
        "    return x + y\n",
        "\n",
        "@tool\n",
        "def multiply(x: float, y: float) -> float:\n",
        "    \"\"\"Multiply 'x' and 'y'.\"\"\"\n",
        "    return x * y\n",
        "\n",
        "@tool\n",
        "def exponentiate(x: float, y: float) -> float:\n",
        "    \"\"\"Raise 'x' to the power of 'y'.\"\"\"\n",
        "    return x ** y\n",
        "\n",
        "@tool\n",
        "def subtract(x: float, y: float) -> float:\n",
        "    \"\"\"Subtract 'x' from 'y'.\"\"\"\n",
        "    return y - x\n",
        "\n",
        "@tool\n",
        "def final_answer(answer: str, tools_used: list[str]) -> str:\n",
        "    \"\"\"Use this tool to provide a final answer to the user.\n",
        "    The answer should be in natural language as this will be provided\n",
        "    to the user directly. The tools_used must include a list of tool\n",
        "    names that were used within the `scratchpad`. You MUST use this tool\n",
        "    to conclude the interaction.\n",
        "    \"\"\"\n",
        "    return {\"answer\": answer, \"tools_used\": tools_used}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmmwW965P4o7"
      },
      "source": [
        "We'll need all of our tools in a list when defining our `agent` and `agent_executor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LunxfvJyP4o7"
      },
      "outputs": [],
      "source": [
        "tools = [add, multiply, exponentiate, subtract, final_answer]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdsscIkrP4o7"
      },
      "source": [
        "### `ChatPromptTemplate`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zT6pWCKP4o7"
      },
      "source": [
        "We will create our `ChatPromptTemplate`, using a system message, chat history, user input, and a scratchpad for intermediate steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "QEDeZUgdP4o7"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"You should use the tools provided even if you are confident about your answer.\"\n",
        "        \"You're a helpful assistant. When answering a user's question \"\n",
        "        \"you should first use one of the tools provided. After using a \"\n",
        "        \"tool the tool output will be provided back to you. You MUST \"\n",
        "        \"then use the final_answer tool to provide a final answer to the user. \"\n",
        "        \"DO NOT use the same tool more than once.\"\n",
        "        \"Use the Final answer tool to conclude the interaction. \"\n",
        "        \"Using the final answer tool is a must.\"\n",
        "\n",
        "    )),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qQsb_BeP4o8"
      },
      "source": [
        "### Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e725jAnP4o8"
      },
      "source": [
        "As before, we will define our `agent` with LCEL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "TdCPU1eFP4o8"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables.base import RunnableSerializable\n",
        "\n",
        "tools = [add, subtract, multiply, exponentiate, final_answer]\n",
        "\n",
        "# define the agent runnable\n",
        "agent: RunnableSerializable = (\n",
        "    {\n",
        "        \"input\": lambda x: x[\"input\"],\n",
        "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
        "    }\n",
        "    | prompt\n",
        "    | llm.bind_tools(tools, tool_choice=\"any\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC9a4CwOP4o8"
      },
      "source": [
        "### Agent Executor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcohW7uHP4o8"
      },
      "source": [
        "Finally, we will create the agent executor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "S0lx-l-xP4o8"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "\n",
        "\n",
        "# create tool name to function mapping\n",
        "name2tool = {tool.name: tool.func for tool in tools}\n",
        "\n",
        "class CustomAgentExecutor:\n",
        "    chat_history: list[BaseMessage]\n",
        "\n",
        "    def __init__(self, max_iterations: int = 3):\n",
        "        self.chat_history = []\n",
        "        self.max_iterations = max_iterations\n",
        "        self.agent: RunnableSerializable = (\n",
        "            {\n",
        "                \"input\": lambda x: x[\"input\"],\n",
        "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "                \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
        "            }\n",
        "            | prompt\n",
        "            | llm.bind_tools(tools, tool_choice=\"required\")  # we're forcing tool use again\n",
        "        )\n",
        "\n",
        "    def invoke(self, input: str) -> dict:\n",
        "        # invoke the agent but we do this iteratively in a loop until\n",
        "        # reaching a final answer\n",
        "        count = 0\n",
        "        agent_scratchpad = []\n",
        "        while count < self.max_iterations:\n",
        "            # invoke a step for the agent to generate a tool call\n",
        "            out = self.agent.invoke({\n",
        "                \"input\": input,\n",
        "                \"chat_history\": self.chat_history,\n",
        "                \"agent_scratchpad\": agent_scratchpad\n",
        "            })\n",
        "            # if the tool call is the final answer tool, we stop\n",
        "            print(out.tool_calls)\n",
        "            if len(out.tool_calls)==0:\n",
        "              count += 1\n",
        "              continue\n",
        "            if out.tool_calls[0][\"name\"] == \"final_answer\":\n",
        "                break\n",
        "            agent_scratchpad.append(out)  # add tool call to scratchpad\n",
        "            # otherwise we execute the tool and add it's output to the agent scratchpad\n",
        "            tool_out = name2tool[out.tool_calls[0][\"name\"]](**out.tool_calls[0][\"args\"])\n",
        "            # add the tool output to the agent scratchpad\n",
        "            action_str = f\"The {out.tool_calls[0]['name']} tool returned {tool_out}\"\n",
        "            agent_scratchpad.append({\n",
        "                \"role\": \"tool\",\n",
        "                \"content\": action_str,\n",
        "                \"tool_call_id\": out.tool_calls[0][\"id\"]\n",
        "            })\n",
        "            # add a print so we can see intermediate steps\n",
        "            print(f\"{count}: {action_str}\")\n",
        "            count += 1\n",
        "        # add the final output to the chat history\n",
        "        final_answer = out.tool_calls[0][\"args\"]\n",
        "        # this is a dictionary, so we convert it to a string for compatibility with\n",
        "        # the chat history\n",
        "        final_answer_str = json.dumps(final_answer)\n",
        "        self.chat_history.append({\"input\": input, \"output\": final_answer_str})\n",
        "        self.chat_history.extend([\n",
        "            HumanMessage(content=input),\n",
        "            AIMessage(content=final_answer_str)\n",
        "        ])\n",
        "        # return the final answer in dict form\n",
        "        return final_answer\n",
        "\n",
        "agent_executor = CustomAgentExecutor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K43Wkw2PP4o8"
      },
      "source": [
        "Our `agent_executor` is now ready to use, let's quickly test it before adding streaming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNOL0o_yP4o8",
        "outputId": "f403c935-2b18-4e16-bcb0-1040f027ac68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "[{'name': 'multiply', 'args': {'x': 5675657, 'y': 687687}, 'id': '1c6a2cdd-56c8-40ad-b483-aefa44f3a303', 'type': 'tool_call'}, {'name': 'final_answer', 'args': {'answer': 'The product of 5675657 and 687687 is 39402188789.'}, 'id': '681444f1-15d7-400f-bd5f-2834df7a1257', 'type': 'tool_call'}]\n",
            "1: The multiply tool returned 3903075535359\n",
            "[{'name': 'final_answer', 'args': {'answer': 'The product of 5675657 and 687687 is 3903075535359.'}, 'id': '05d0cec5-7d7f-4580-a6c6-fb75841bdf57', 'type': 'tool_call'}]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'The product of 5675657 and 687687 is 3903075535359.'}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "agent_executor.invoke(input=\"What is 5675657*687687\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdifQE5SP4o8"
      },
      "source": [
        "Let's modify our `agent_executor` to use streaming and parse the streamed output into a format that we can more easily work with.\n",
        "\n",
        "First, when streaming with our custom agent executor we will need to pass our callback handler to the agent on every new invocation. To make this simpler we can make the `callbacks` field a configurable field and this will allow us to initialize the agent using the `with_config` method, allowing us to pass the callback handler to the agent with every invocation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "lOZDrXCuP4o9"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import ConfigurableField\n",
        "\n",
        "llm = ChatOllama(\n",
        "    model=\"mistral\",\n",
        "    temperature=0.0,\n",
        "    streaming=True\n",
        ").configurable_fields(\n",
        "    callbacks=ConfigurableField(\n",
        "        id=\"callbacks\",\n",
        "        name=\"callbacks\",\n",
        "        description=\"A list of callbacks to use for streaming\",\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7k1thVGP4o9"
      },
      "source": [
        "We reinitialize our `agent`, nothing changes here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "UFVoYB6OP4o9"
      },
      "outputs": [],
      "source": [
        "# define the agent runnable\n",
        "agent: RunnableSerializable = (\n",
        "    {\n",
        "        \"input\": lambda x: x[\"input\"],\n",
        "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
        "    }\n",
        "    | prompt\n",
        "    | llm.bind_tools(tools, tool_choice=\"any\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztlchX8dP4o9"
      },
      "source": [
        "Now, we will define our _custom_ callback handler. This will be a queue callback handler that will allow us to stream the output of the agent through an `asyncio.Queue` object and yield the tokens as they are generated elsewhere."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "xPpdIQxyP4o9"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from langchain.callbacks.base import AsyncCallbackHandler\n",
        "\n",
        "\n",
        "class QueueCallbackHandler(AsyncCallbackHandler):\n",
        "    \"\"\"Callback handler that puts tokens into a queue.\"\"\"\n",
        "\n",
        "    def __init__(self, queue: asyncio.Queue):\n",
        "        self.queue = queue\n",
        "        self.final_answer_seen = False\n",
        "\n",
        "    async def __aiter__(self):\n",
        "        while True:\n",
        "            if self.queue.empty():\n",
        "                await asyncio.sleep(0.1)\n",
        "                continue\n",
        "            token_or_done = await self.queue.get()\n",
        "\n",
        "            if token_or_done == \"<<DONE>>\":\n",
        "                # this means we're done\n",
        "                return\n",
        "            if token_or_done:\n",
        "                yield token_or_done\n",
        "\n",
        "    async def on_llm_new_token(self, *args, **kwargs) -> None:\n",
        "        \"\"\"Put new token in the queue.\"\"\"\n",
        "        #print(f\"on_llm_new_token: {args}, {kwargs}\")\n",
        "        chunk = kwargs.get(\"chunk\")\n",
        "        if chunk:\n",
        "            # check for final_answer tool call\n",
        "            if tool_calls := chunk.message.additional_kwargs.get(\"tool_calls\"):\n",
        "                if tool_calls[0][\"function\"][\"name\"] == \"final_answer\":\n",
        "                    # this will allow the stream to end on the next `on_llm_end` call\n",
        "                    self.final_answer_seen = True\n",
        "        self.queue.put_nowait(kwargs.get(\"chunk\"))\n",
        "        return\n",
        "\n",
        "    async def on_llm_end(self, *args, **kwargs) -> None:\n",
        "        \"\"\"Put None in the queue to signal completion.\"\"\"\n",
        "        #print(f\"on_llm_end: {args}, {kwargs}\")\n",
        "        # this should only be used at the end of our agent execution, however LangChain\n",
        "        # will call this at the end of every tool call, not just the final tool call\n",
        "        # so we must only send the \"done\" signal if we have already seen the final_answer\n",
        "        # tool call\n",
        "        if self.final_answer_seen:\n",
        "            self.queue.put_nowait(\"<<DONE>>\")\n",
        "        else:\n",
        "            self.queue.put_nowait(\"<<STEP_END>>\")\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsjISrlRP4o9"
      },
      "source": [
        "We can see how this works together in our `agent` invocation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADNwPcaVP4o9",
        "outputId": "32e0088e-8d9c-4c6d-9498-1b9f36601cb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='' additional_kwargs={} response_metadata={} id='run-35a522c7-7861-403c-93b6-5a916a18de00' tool_calls=[{'name': 'multiply', 'args': {'x': 57656756, 'y': 6757655}, 'id': '4d9ce070-013b-43be-82df-08234e26972b', 'type': 'tool_call'}] tool_call_chunks=[{'name': 'multiply', 'args': '{\"x\": 57656756, \"y\": 6757655}', 'id': '4d9ce070-013b-43be-82df-08234e26972b', 'index': None, 'type': 'tool_call_chunk'}]\n",
            "content='' additional_kwargs={} response_metadata={} id='run-35a522c7-7861-403c-93b6-5a916a18de00' tool_calls=[{'name': 'final_answer', 'args': {'answer': 'The product of 57656756 and 6757655 is 3890214028816100.', 'tools_used': ['multiply']}, 'id': 'b3d860e2-855b-419c-827a-ac6e95d44c20', 'type': 'tool_call'}] tool_call_chunks=[{'name': 'final_answer', 'args': '{\"answer\": \"The product of 57656756 and 6757655 is 3890214028816100.\", \"tools_used\": [\"multiply\"]}', 'id': 'b3d860e2-855b-419c-827a-ac6e95d44c20', 'index': None, 'type': 'tool_call_chunk'}]\n",
            "content='' additional_kwargs={} response_metadata={'model': 'mistral', 'created_at': '2025-03-24T15:15:00.02675753Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6462483475, 'load_duration': 7733233, 'prompt_eval_count': 541, 'prompt_eval_duration': 8235911, 'eval_count': 265, 'eval_duration': 6443622717, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-35a522c7-7861-403c-93b6-5a916a18de00' usage_metadata={'input_tokens': 541, 'output_tokens': 265, 'total_tokens': 806}\n"
          ]
        }
      ],
      "source": [
        "queue = asyncio.Queue()\n",
        "streamer = QueueCallbackHandler(queue)\n",
        "\n",
        "tokens = []\n",
        "\n",
        "async def stream(query: str):\n",
        "    response = agent.with_config(\n",
        "        callbacks=[streamer]\n",
        "    )\n",
        "    async for token in response.astream({\n",
        "        \"input\": query,\n",
        "        \"chat_history\": [],\n",
        "        \"agent_scratchpad\": []\n",
        "    }):\n",
        "        tokens.append(token)\n",
        "        print(token, flush=True)\n",
        "\n",
        "await stream(\"What is 57656756*6757655\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2LLJ9ESKMmI",
        "outputId": "4039b9da-e5a2-4045-c0b0-ea8683600822"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model': 'mistral', 'created_at': '2025-03-24T15:15:00.02675753Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6462483475, 'load_duration': 7733233, 'prompt_eval_count': 541, 'prompt_eval_duration': 8235911, 'eval_count': 265, 'eval_duration': 6443622717, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-35a522c7-7861-403c-93b6-5a916a18de00', tool_calls=[{'name': 'multiply', 'args': {'x': 57656756, 'y': 6757655}, 'id': '4d9ce070-013b-43be-82df-08234e26972b', 'type': 'tool_call'}, {'name': 'final_answer', 'args': {'answer': 'The product of 57656756 and 6757655 is 3890214028816100.', 'tools_used': ['multiply']}, 'id': 'b3d860e2-855b-419c-827a-ac6e95d44c20', 'type': 'tool_call'}], usage_metadata={'input_tokens': 541, 'output_tokens': 265, 'total_tokens': 806}, tool_call_chunks=[{'name': 'multiply', 'args': '{\"x\": 57656756, \"y\": 6757655}', 'id': '4d9ce070-013b-43be-82df-08234e26972b', 'index': None, 'type': 'tool_call_chunk'}, {'name': 'final_answer', 'args': '{\"answer\": \"The product of 57656756 and 6757655 is 3890214028816100.\", \"tools_used\": [\"multiply\"]}', 'id': 'b3d860e2-855b-419c-827a-ac6e95d44c20', 'index': None, 'type': 'tool_call_chunk'}])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "tk = tokens[0]\n",
        "\n",
        "for token in tokens[1:]:\n",
        "    tk += token\n",
        "\n",
        "tk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7upXtKmP4o9"
      },
      "source": [
        "Now we're seeing that the output is being streamed token-by-token. Because we're being streamed a tool call the `content` field is empty. Instead, we can see the tokens being added inside the `tool_calls` fields, within `id`, `function.name`, and `function.arguments`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "YtHO7IzdP4o9"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "class CustomAgentExecutor:\n",
        "    chat_history: list[BaseMessage]\n",
        "\n",
        "    def __init__(self, max_iterations: int = 3):\n",
        "        self.chat_history = []\n",
        "        self.max_iterations = max_iterations\n",
        "        self.agent: RunnableSerializable = (\n",
        "            {\n",
        "                \"input\": lambda x: x[\"input\"],\n",
        "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "                \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
        "            }\n",
        "            | prompt\n",
        "            | llm.bind_tools(tools, tool_choice=\"any\")  # we're forcing tool use again\n",
        "        )\n",
        "\n",
        "    async def invoke(self, input: str, streamer: QueueCallbackHandler, verbose: bool = False) -> dict:\n",
        "        # invoke the agent but we do this iteratively in a loop until\n",
        "        # reaching a final answer\n",
        "        count = 0\n",
        "        agent_scratchpad = []\n",
        "        while count < self.max_iterations:\n",
        "            # invoke a step for the agent to generate a tool call\n",
        "            async def stream(query: str):\n",
        "                response = self.agent.with_config(\n",
        "                    callbacks=[streamer]\n",
        "                )\n",
        "                # we initialize the output dictionary that we will be populating with\n",
        "                # our streamed output\n",
        "                output = None\n",
        "                # now we begin streaming\n",
        "                async for token in response.astream({\n",
        "                    \"input\": query,\n",
        "                    \"chat_history\": self.chat_history,\n",
        "                    \"agent_scratchpad\": agent_scratchpad\n",
        "                }):\n",
        "                    if output is None:\n",
        "                        output = token\n",
        "                    else:\n",
        "                        # we can just add the tokens together as they are streamed and\n",
        "                        # we'll have the full response object at the end\n",
        "                        output += token\n",
        "                    if token.content != \"\":\n",
        "                        # we can capture various parts of the response object\n",
        "                        if verbose: print(f\"content: {token.content}\", flush=True)\n",
        "                    tool_calls = getattr(token, \"tool_calls\", None)\n",
        "                    if tool_calls:\n",
        "                        if verbose: print(f\"tool_calls: {tool_calls}\", flush=True)\n",
        "                        tool_name = tool_calls[0][\"name\"]\n",
        "                        if tool_name:\n",
        "                            if verbose: print(f\"tool_name: {tool_name}\", flush=True)\n",
        "                        arg = tool_calls[0][\"args\"]\n",
        "                        if arg != \"\":\n",
        "                            if verbose: print(f\"arg: {arg}\", flush=True)\n",
        "                return AIMessage(\n",
        "                    content=output.content,\n",
        "                    tool_calls=output.tool_calls,\n",
        "                    tool_call_id = output.tool_calls[0][\"id\"] if output.tool_calls else None\n",
        "\n",
        "                )\n",
        "\n",
        "            tool_call = await stream(query=input)\n",
        "            if tool_call.tool_call_id==None:\n",
        "                count += 1\n",
        "                continue\n",
        "            # add initial tool call to scratchpad\n",
        "            agent_scratchpad.append(tool_call)\n",
        "            # otherwise we execute the tool and add it's output to the agent scratchpad\n",
        "            tool_name = tool_call.tool_calls[0][\"name\"]\n",
        "            tool_args = tool_call.tool_calls[0][\"args\"]\n",
        "            tool_call_id = tool_call.tool_call_id\n",
        "            tool_out = name2tool[tool_name](**tool_args)\n",
        "            # add the tool output to the agent scratchpad\n",
        "            tool_exec = ToolMessage(\n",
        "                content=f\"{tool_out}\",\n",
        "                tool_call_id=tool_call_id\n",
        "            )\n",
        "            agent_scratchpad.append(tool_exec)\n",
        "            count += 1\n",
        "            # if the tool call is the final answer tool, we stop\n",
        "            if tool_name == \"final_answer\":\n",
        "                break\n",
        "        # add the final output to the chat history, we only add the \"answer\" field\n",
        "        if tool_out!=None:\n",
        "          final_answer = str(tool_out)\n",
        "        else:\n",
        "          final_answer = tool_call.content\n",
        "        self.chat_history.extend([\n",
        "            HumanMessage(content=input),\n",
        "            AIMessage(content=final_answer)\n",
        "        ])\n",
        "        # return the final answer in dict form\n",
        "        return tool_args\n",
        "\n",
        "agent_executor = CustomAgentExecutor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0HKjOO5P4o9"
      },
      "source": [
        "We've added a few `print` statements to help us see what is being output, we activate those by setting `verbose=True`. Let's see what is returned:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDf6uLhPP4o9",
        "outputId": "0fba4533-a2b8-4ae2-c339-9b881f603a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content:  To find the product of two large numbers, I will use the multiply tool. Here's how it works:\n",
            "\n",
            "1. Use the multiply tool with 'x' as 67678 and 'y' as 8687687687.\n",
            "2. The output from the multiply tool will be the product of the two numbers.\n",
            "3. Finally, use the final_answer tool to provide a clear and concise answer to the user.\n",
            "\n",
            "Here's the code:\n",
            "```javascript\n",
            "let result = add({x: multiply({x: 67678, y: 8687687687}, {x: 1, y: 1})[0], y: 0});\n",
            "final_answer({answer: `The product of 67678 and 8687687687 is ${result}`, tools_used: [\"multiply\"]});\n",
            "```\n",
            "tool_calls: [{'name': 'multiply', 'args': {'x': 67678, 'y': 8687687687}, 'id': '66385917-86a2-44e8-9b98-bcd0c943e056', 'type': 'tool_call'}]\n",
            "tool_name: multiply\n",
            "arg: {'x': 67678, 'y': 8687687687}\n",
            "tool_calls: [{'name': 'final_answer', 'args': {'answer': 'The product of the two numbers is [result from multiply tool]', 'tools_used': ['multiply']}, 'id': '51b99c0c-b530-42c4-bcbe-870a1a5d9183', 'type': 'tool_call'}]\n",
            "tool_name: final_answer\n",
            "arg: {'answer': 'The product of the two numbers is [result from multiply tool]', 'tools_used': ['multiply']}\n",
            "content: 5,879,653,272,078,600 is the result of multiplying 67,678 and 86,876,876,876.\n"
          ]
        }
      ],
      "source": [
        "queue = asyncio.Queue()\n",
        "streamer = QueueCallbackHandler(queue)\n",
        "\n",
        "out = await agent_executor.invoke(\"What is 67678*8687687687\", streamer, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK8HcGbeP4o-"
      },
      "source": [
        "We can see what is being output through the `verbose=True` flag. However, if we do _not_ `print` the output, we will see nothing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "jQhdzX4YP4o-"
      },
      "outputs": [],
      "source": [
        "queue = asyncio.Queue()\n",
        "streamer = QueueCallbackHandler(queue)\n",
        "\n",
        "out = await agent_executor.invoke(\"What is 67678*868767\", streamer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV7-zSOuP4o-"
      },
      "source": [
        "Although we see nothing, it does not mean that nothing is being returned to us - we're just not using our callback handler and `asyncio.Queue`. To use these we create an `asyncio` task, iterate over the `__aiter__` method of our `streamer` object, and await the task, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxi44XjNP4o-",
        "outputId": "b2ee717b-155f-41ea-cc8f-b6ea60dfebeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-4fa4fe1e-e512-4c3b-9082-637f0619c7aa', tool_calls=[{'name': 'multiply', 'args': {'x': 67678, 'y': 868767}, 'id': 'c95beb6f-e4ad-439f-bfbf-d23abb29dca9', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'multiply', 'args': '{\"x\": 67678, \"y\": 868767}', 'id': 'c95beb6f-e4ad-439f-bfbf-d23abb29dca9', 'index': None, 'type': 'tool_call_chunk'}])\n",
            "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-4fa4fe1e-e512-4c3b-9082-637f0619c7aa', tool_calls=[{'name': 'final_answer', 'args': {'answer': 'The product of 67678 and 868767 is 58796413026.', 'tools_used': ['multiply']}, 'id': 'dff11d58-d38e-4284-9b94-4f823d3b921f', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'final_answer', 'args': '{\"answer\": \"The product of 67678 and 868767 is 58796413026.\", \"tools_used\": [\"multiply\"]}', 'id': 'dff11d58-d38e-4284-9b94-4f823d3b921f', 'index': None, 'type': 'tool_call_chunk'}])\n",
            "generation_info={'model': 'mistral', 'created_at': '2025-03-24T16:15:10.662274312Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5017104980, 'load_duration': 8554880, 'prompt_eval_count': 607, 'prompt_eval_duration': 554593659, 'eval_count': 189, 'eval_duration': 4442966703, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model': 'mistral', 'created_at': '2025-03-24T16:15:10.662274312Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5017104980, 'load_duration': 8554880, 'prompt_eval_count': 607, 'prompt_eval_duration': 554593659, 'eval_count': 189, 'eval_duration': 4442966703, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-4fa4fe1e-e512-4c3b-9082-637f0619c7aa', usage_metadata={'input_tokens': 607, 'output_tokens': 189, 'total_tokens': 796})\n",
            "<<STEP_END>>\n",
            "text='58796413026 is the correct answer for the multiplication of 67678 and 868767.' generation_info={'model': 'mistral', 'created_at': '2025-03-24T16:15:11.600686016Z', 'done': True, 'done_reason': 'stop', 'total_duration': 916827561, 'load_duration': 13489343, 'prompt_eval_count': 198, 'prompt_eval_duration': 28335395, 'eval_count': 36, 'eval_duration': 870658376, 'message': Message(role='assistant', content='58796413026 is the correct answer for the multiplication of 67678 and 868767.', images=None, tool_calls=None)} message=AIMessageChunk(content='58796413026 is the correct answer for the multiplication of 67678 and 868767.', additional_kwargs={}, response_metadata={'model': 'mistral', 'created_at': '2025-03-24T16:15:11.600686016Z', 'done': True, 'done_reason': 'stop', 'total_duration': 916827561, 'load_duration': 13489343, 'prompt_eval_count': 198, 'prompt_eval_duration': 28335395, 'eval_count': 36, 'eval_duration': 870658376, 'message': Message(role='assistant', content='58796413026 is the correct answer for the multiplication of 67678 and 868767.', images=None, tool_calls=None)}, id='run-1322c70c-c247-4a26-a22c-bd19e5644785', usage_metadata={'input_tokens': 198, 'output_tokens': 36, 'total_tokens': 234})\n",
            "<<STEP_END>>\n",
            "text='58796413026 is the correct answer for the multiplication of 67678 and 868767.' generation_info={'model': 'mistral', 'created_at': '2025-03-24T16:15:12.446447439Z', 'done': True, 'done_reason': 'stop', 'total_duration': 825636537, 'load_duration': 11734634, 'prompt_eval_count': 198, 'prompt_eval_duration': 8049643, 'eval_count': 36, 'eval_duration': 798208353, 'message': Message(role='assistant', content='58796413026 is the correct answer for the multiplication of 67678 and 868767.', images=None, tool_calls=None)} message=AIMessageChunk(content='58796413026 is the correct answer for the multiplication of 67678 and 868767.', additional_kwargs={}, response_metadata={'model': 'mistral', 'created_at': '2025-03-24T16:15:12.446447439Z', 'done': True, 'done_reason': 'stop', 'total_duration': 825636537, 'load_duration': 11734634, 'prompt_eval_count': 198, 'prompt_eval_duration': 8049643, 'eval_count': 36, 'eval_duration': 798208353, 'message': Message(role='assistant', content='58796413026 is the correct answer for the multiplication of 67678 and 868767.', images=None, tool_calls=None)}, id='run-89d40803-164a-4fa0-afd5-b643038db30c', usage_metadata={'input_tokens': 198, 'output_tokens': 36, 'total_tokens': 234})\n",
            "<<STEP_END>>\n"
          ]
        }
      ],
      "source": [
        "queue = asyncio.Queue()\n",
        "streamer = QueueCallbackHandler(queue)\n",
        "\n",
        "task = asyncio.create_task(agent_executor.invoke(\"What is 67678*868767\", streamer))\n",
        "\n",
        "async for token in streamer:\n",
        "    print(token, flush=True)\n",
        "\n",
        "await task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr0-ZMqaP4o-"
      },
      "source": [
        "Although this seems like a lot of work, we're now streaming tokens in a way that allows us to pass these tokens on to other parts of our code - such as through a websocket, streamed API response, or some downstream processing.\n",
        "\n",
        "Let's try this out, we'll put together some simple post-processing to allow us to more nicely format the streamed output from out agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy-yvF62P4o-",
        "outputId": "01c133d6-a64c-4c6e-b613-fe28414b0dfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calling add...\n",
            "{\"x\":10,\"y\":10}\n",
            "\n",
            "Calling final_answer...\n",
            "{\"answer\":\"10 + 10 equals 20.\",\"tools_used\":[\"functions.add\"]}"
          ]
        }
      ],
      "source": [
        "queue = asyncio.Queue()\n",
        "streamer = QueueCallbackHandler(queue)\n",
        "\n",
        "task = asyncio.create_task(agent_executor.invoke(\"What is 10 + 10\", streamer))\n",
        "\n",
        "async for token in streamer:\n",
        "    # first identify if we have a <<STEP_END>> token\n",
        "    if token == \"<<STEP_END>>\":\n",
        "        print(\"\\n\", flush=True)\n",
        "    # we'll first identify if the token is a tool call\n",
        "    elif tool_calls := token.message.additional_kwargs.get(\"tool_calls\"):\n",
        "        # if we have a tool call with a tool name, we'll print it\n",
        "        if tool_name := tool_calls[0][\"function\"][\"name\"]:\n",
        "            print(f\"Calling {tool_name}...\", flush=True)\n",
        "        # if we have a tool call with arguments, we ad them to our args string\n",
        "        if tool_args := tool_calls[0][\"function\"][\"arguments\"]:\n",
        "            print(f\"{tool_args}\", end=\"\", flush=True)\n",
        "\n",
        "_ = await task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1mv4tmEP4o-"
      },
      "source": [
        "With that we've produced a nice streaming output within our notebook - which ofcourse can be applied with very similar logic elsewhere, such as within a more polished web app."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}